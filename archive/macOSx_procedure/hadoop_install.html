<html>
<head>
<title>HADOOP SETUP</title>
</head>

<body>
<h2>HADOOP SETUP</h2>
<fieldset>
<p>
There is an excellent procedure for setting up Hadoop 2.6 on Mac OS X Yosemite.  It was created by Yaonan Zhong and it is located at <a href='http://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html' target='_blank'>http://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html</a>.  The steps are re-iterated below with some modifications where I encountered a problem or for additional clarification.

<ul type='disc'>
  <li>Make sure you have java installed.  I utilized JDK 7 for my setup.<br /><br /></li>
    <ul type='circle'>
      <li>To download Java, go to <a href='http://www.oracle.com/technetwork/java/javasebusiness/downloads/java-ee-sdk-6u3-jdk-7-downloads-439818.html' target='_blank'>http://www.oracle.com/technetwork/java/javasebusiness/downloads/java-ee-sdk-6u3-jdk-7-downloads-439818.html</a>.
      <br /><br /></li>
      <li>To install Java, double-click on the downloaded .dmg file and follow the instructions.</li>
      <br /></li>
      <li>To verify proper installation, open a terminal window and type "java -version".</li>
      <br /></li>
    </ul>

  <li>SSH<br />
First enable Remote Login in System Preference -> Sharing.<br />
<br />
Now check that you can ssh to the localhost without a passphrase:<br />
 &nbsp; &nbsp; &nbsp; $ ssh localhost<br /><br />
If you cannot ssh to localhost without a passphrase, execute the following commands:<br />
 &nbsp; &nbsp; &nbsp; $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa<br />
 &nbsp; &nbsp; &nbsp; $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys<br />
<br /></li>

  <li>Get a Hadoop distribution.  Caution:  Get Hadoop version 2.6.*.  I initially tried Hadoop 2.7.3 and encountered lots of problems.  I spent a lot of time trying debug the Hadoop 2.7.3 installation and while inspecting the Hadoop 2.7.3 shell setup scripts, I discovered some environment variables were required to be specified.  At this point, I realized that 2.7.3 may not be compatible with Mac OS X 10.10 Yosemite. Upon switching to Hadoop 2.6.5, I encountered no problems.  Installation was a breeze!<br /> 
<br />
You can download Haddop 2.6.* from Apache Download Mirror at <a href='http://hadoop.apache.org/releases.html' target='_blank'>http://hadoop.apache.org/releases.html</a>.<br /><br /></li>

  <li>Prepare to start the Hadoop cluster
    <ul type='circle'>

      <li>Unpack the downloaded Hadoop distribution.
          <br /> &nbsp; tar -xvf hadoop-2.6.5.tar
          <br />
          <br /> &nbsp; Move the directory containing your Hadoop distribution to /usr/local
          <br /> &nbsp; &nbsp; &nbsp; For example:  mv hadoop-2.6.5 /usr/local
          <br />
          <br /> &nbsp; Then create a soft link pointing to this directory.
          <br /> &nbsp; &nbsp; &nbsp; For example:  ln -s /usr/local/hadoop-2.6.5 /usr/local/hadoop
          <br />
          <br /> &nbsp; So, /usr/local/hadoop will be your Hadoop distribution directory
          <br /><br /></li>

      <li>Run the following command to figure out where is your Java home directory:
          <br />Determine what is your java home directory:
          <br /> &nbsp; $ /usr/libexec/java_home
          <br /> &nbsp; /Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home
          <br /><br /></li>

      <li>In the distribution, edit the file etc/hadoop/hadoop-env.sh to define some parameters as follows:
          <br /> &nbsp; # set to the root of your Java installation
          <br /> &nbsp; &nbsp; &nbsp; export JAVA_HOME={your java home directory}
          <br />
          <br /> &nbsp; # set to the root of your Hadoop installation
          <br /> &nbsp; &nbsp; &nbsp; export HADOOP_PREFIX={your hadoop distribution directory}
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;In my case, it was /usr/local/hadoop
          <br /><br /></li>

      <li>Try the following command:
          <br /> &nbsp; $ cd {your hadoop distribution directory}
          <br >
          <br /> &nbsp; $ bin/hadoop
          <br />
          <br /> &nbsp; This will display the usage documentation for the hadoop script.
          <br /> &nbsp; 
          <br /> &nbsp; Now you are ready to start your Hadoop cluster in one of the three supported modes:
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Standalone mode
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Pseudo-distributed mode &nbsp; &nbsp; &nbsp; -&gt; procedures to follow are for this mode
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; fully-distributed mode
          <br />
          <br /> &nbsp; We will go through pseudo-distributed mode and run a MapReduce job on YARN here. In this mode, Hadoop runs on a single node and each Hadoop daemon runs in a separate Java process.<br />
          <br /></li>
    </ul>
    <br />
  </li>
  

  <li>Configuration
          <br /> &nbsp; Edit following config files in your Hadoop directory
          <br />
    <ul type='circle'>
          <li>$HADOOP_PREFIX/etc/hadoop/core-site.xml:
          <br /> &nbsp; &lt;configuration&gt;
          <br /> &nbsp;     &lt;property&gt;
          <br /> &nbsp;         &lt;name&gt;fs.defaultFS&lt;/name&gt;
          <br /> &nbsp;         &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
          <br /> &nbsp;     &lt;/property&gt;
          <br /> &nbsp; &lt;/configuration&gt;
          <br /><br /></li>

      <li>$HADOOP_PREFIX/etc/hadoop/hdfs-site.xml:
          <br /> &nbsp; &lt;configuration&gt;
          <br /> &nbsp;     &lt;property&gt;
          <br /> &nbsp;         &lt;name&gt;dfs.replication&lt;/name&gt;
          <br /> &nbsp;         &lt;value&gt;1&lt;/value&gt;
          <br /> &nbsp;     &lt;/property&gt;
          <br /> &nbsp; &lt;/configuration&gt;
          <br /><br /></li>

      <li>$HADOOP_PREFIX/etc/hadoop/mapred-site.xml:
          <br /> &nbsp; &lt;configuration&gt;
          <br /> &nbsp;     &lt;property&gt;
          <br /> &nbsp;         &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
          <br /> &nbsp;         &lt;value&gt;yarn&lt;/value&gt;
          <br /> &nbsp;     &lt;/property&gt;
          <br /> &nbsp; &lt;/configuration&gt;
          <br /><br /></li>

      <li>$HADOOP_PREFIX/etc/hadoop/yarn-site.xml:
          <br /> &nbsp; &lt;configuration&gt;
          <br /> &nbsp;     &lt;property&gt;
          <br /> &nbsp;         &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
          <br /> &nbsp;         &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
          <br /> &nbsp;     &lt;/property&gt;
          <br /> &nbsp;     &lt;property&gt;
          <br /> &nbsp;         &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
          <br /> &nbsp;         &lt;value&gt;127.0.0.1&lt;/value&gt;
          <br /> &nbsp;     &lt;/property&gt;
          <br /> &nbsp; &lt;/configuration&gt;
          <br /><br /></li>
    </ul>
  </li>


  <li>Execution
    <ul type='circle'>
      <li>Format and start HDFS and YARN
          <br />
          <br />$ cd $HADOOP_PREFIX
          <br />
          <br />Format the filesystem:
          <br /> &nbsp; &nbsp; &nbsp; $ bin/hdfs namenode -format
          <br />
          <br />Start NameNode daemon and DataNode daemon:
          <br /> &nbsp; &nbsp; &nbsp; $ sbin/start-dfs.sh
          <br />
          <br />Now you can browse the web interface for the NameNode at - http://localhost:50070/
          <br />
          <br />Make the HDFS directories required to execute MapReduce jobs:
          <br /> &nbsp; &nbsp; &nbsp; $ bin/hdfs dfs -mkdir /user
          <br /> &nbsp; &nbsp; &nbsp; $ bin/hdfs dfs -mkdir /user/{username} #make sure you add correct username here
          <br />
          <br />Start ResourceManager daemon and NodeManager daemon:
          <br /> &nbsp; &nbsp; &nbsp; $ sbin/start-yarn.sh
          <br />
          <br />Browse the web interface for the ResourceManager at - http://localhost:8088/
          <br /><br /></li>

      <li>Test out some dfs commands: 
          <br />
          <br /> &nbsp; &nbsp; &nbsp; $ $HADOOP_PREFIX/bin/hdfs dfs -{dfs command} $HADOOP_PREFIX/etc/hadoop hdfs://localhost:9000/
          <br /> &nbsp; &nbsp; &nbsp; Note:  The hdfs://localhost:9000/ was defined in your core-site.xml file.
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {dfs command} can be ls, mkdir, get, put, cp, etc. 
          <br />
          <br />Run the different dfs commands to get familiar with how they work.  <a href='hadoop_dfs_cmds.html' target='_blank'>Refer to sample dfs commands.</a>
          <br /><br /></li>
    
      <li>Now to run your first mapreduce job
          <br />Check to make sure namenode, secondary namenode, datanode, ResourceManager, and NodeManager processes are running:
          <br /> &nbsp; &nbsp; &nbsp; root# Jps
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1624 DataNode
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2355 Jps
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1716 SecondaryNameNode
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2239 ResourceManager
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2315 NodeManager
          <br /> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1552 NameNode
          <br />
          <br />Setup your mapreduce input and output directories:
          <br /> &nbsp; &nbsp; &nbsp; bin/hdfs dfs -mkdir -p hdfs://localhost:9000/user/root/input
          <br /> &nbsp; &nbsp; &nbsp; bin/hdfs dfs -mkdir hdfs://localhost:9000/user/root/output
          <br />
          <br />Copy the input files into the distributed filesystem:
          <br /> &nbsp; &nbsp; &nbsp; $ $HADOOP_PREFIX/bin/hdfs dfs -put $HADOOP_PREFIX/etc/hadoop hdfs://localhost:9000/
          <br />
          <br />Now run your mapreduce job:
          <br /> &nbsp; &nbsp; &nbsp; bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar wordcount input output 'dfs[a-z.]+'
          <br /> &nbsp; &nbsp; &nbsp; The output from this command should be as shown in <a href='hadoop_mapreduce_output_smpl.txt' href='_blank'>hadoop_mapreduce_output_smpl.txt</a>
          <br /><br /></li>

  </li>
  </ul>
</p>
</fieldset>
</body>
</html>
